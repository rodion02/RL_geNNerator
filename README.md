# RL_geNNerator
Just the best
Алгоритмы работы RL системы 

# Варианты:

## 1) алгоритм генерации нейросети послойно 

- агент получается на вход в качестве состояния среды список последних метрик за последние N обучений
Обучение представляет из себя создание нейросети+ обучение N эпох с отслеживанием метрик
Как результат обучения агент получает лучшие значения метрик

По результатам предыдущих обучений агент принимает решение о том, как слой добавить в конце предыдущей нейросети

## 2) алгоритм генерации нейросети полностью

Входные данные такие же как и в пункт 1
Отличие заключается лишь в том, что агент сразу простраивает всю нейросетевую архитектуру вместо того, чтобы добавлять новый слой в конец

При таком подходе можно добавить в качестве награды уменьшение нейросети количество слоев ( оптимизация архитектур)

# СТРУКТУРА КЛАССА СРЕДЫ (observation):
Init - объявление переменных 
- обязательные параметры 
* Action space
* Reservation space
Как среда будет выдавать полные данные для агента 
Reset - очистить все изменения в среде
Step - САМЫЙ ВАЖНЫЙ
В него передаёт action от агента
В нем генерируется новая наблюдение для агента и награда
Render - метод, который генерирует графическую часть

# ОБЪЯСНЕНИЕ РАБОТЫ СТЭП ШАГА:
получаем на вход action
## 1) если мы генерируем послойно:
Мы преобразовываем действие агента в новый слой, добавляем его в конец текущей архитектуры
Строим нейросеть заново с учётом добавленного слоя
Проводим обучение полученной модели, трассируя метрики
После чего, на основании результатов обучения вычисляется награда 
Полученные метрики добавляются в конец массива из последних результатов обучений 
И этот список передаёт как наблюдение агенту вместе с наградой

## 2) если мы генерируем архитектуру полностью
Мы преобразовываем action от агента в нейросетевую архитектуру
Проводим обучение полученной модели
И возвращаем тоже самое что в пункте 1, только от агента мы получаем нейросетевую архитектуру целиком

